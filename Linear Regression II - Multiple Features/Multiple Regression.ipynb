{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./boston_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = data.drop([\"TARGET\"], axis=1)\n",
    "y = data[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = ((df_X - df_X.mean())/df_X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[\"x_0\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2       x_3       x_4       x_5       x_6       x_7  \\\n",
       "0 -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1 -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2 -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3 -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4 -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "\n",
       "        x_8       x_9      x_10      x_11      x_12      x_13  x_0  \n",
       "0  0.140075 -0.981871 -0.665949 -1.457558  0.440616 -1.074499    1  \n",
       "1  0.556609 -0.867024 -0.986353 -0.302794  0.440616 -0.491953    1  \n",
       "2  0.556609 -0.867024 -0.986353 -0.302794  0.396035 -1.207532    1  \n",
       "3  1.076671 -0.752178 -1.105022  0.112920  0.415751 -1.360171    1  \n",
       "4  1.076671 -0.752178 -1.105022  0.112920  0.440616 -1.025487    1  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    hy = np.zeros(X.shape[0])\n",
    "    for i in range(len(theta)):\n",
    "        hy+=theta[i]*X[:, i]\n",
    "    return hy\n",
    "\n",
    "def error(X, theta, y_true):\n",
    "    y_pred = hypothesis(X, theta)\n",
    "    \n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def grad(X, y, j, theta):\n",
    "    # gradient of loss wrt jth theta\n",
    "    y_pred = hypothesis(X, theta)\n",
    "    return -2*((y - y_pred)*X[:, j]).mean()\n",
    "\n",
    "def gradient_descent(X, y, lr=0.001, n_epochs=1000):\n",
    "    theta = np.random.randn(X.shape[1])\n",
    "    n_features = X.shape[1]\n",
    "    # create an list and append the errors for each epoch and at the end of training visualize the values of these errors.\n",
    "    for i in range(n_epochs):\n",
    "        gradients = np.zeros_like(theta)\n",
    "        # Try to implement the gradients array without loops. Hint: Use numpy\n",
    "        for j in range(n_features):\n",
    "            gradients[j] = grad(X, y, j, theta)\n",
    "        \n",
    "        theta = theta - lr*gradients\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Error at epoch {i+1} is {error(X, theta, y)} with r2-score {r2_score(hypothesis(X, theta), y)}\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at epoch 1 is 551.786551189593 with r2-score -553.6240846479426\n",
      "Error at epoch 101 is 358.660965519995 with r2-score -324.85530823748627\n",
      "Error at epoch 201 is 246.66072477828268 with r2-score -192.18434212327696\n",
      "Error at epoch 301 is 173.3627711935783 with r2-score -105.35854378679628\n",
      "Error at epoch 401 is 124.50398919617845 with r2-score -47.48240202289353\n",
      "Error at epoch 501 is 91.80781432208542 with r2-score -8.75183251645213\n",
      "Error at epoch 601 is 69.87827626431923 with r2-score 17.225013437581683\n",
      "Error at epoch 701 is 55.135270309877 with r2-score 34.68898342952231\n",
      "Error at epoch 801 is 45.19555466810777 with r2-score 46.463169523775186\n",
      "Error at epoch 901 is 38.47042275867256 with r2-score 54.429489433103484\n",
      "Error at epoch 1001 is 33.89968116272784 with r2-score 59.843805504002304\n",
      "Error at epoch 1101 is 30.775127087613356 with r2-score 63.54502618956768\n",
      "Error at epoch 1201 is 28.623224749987212 with r2-score 66.09408287217495\n",
      "Error at epoch 1301 is 27.127031748417533 with r2-score 67.86641273233427\n",
      "Error at epoch 1401 is 26.07418946248419 with r2-score 69.11356722339286\n",
      "Error at epoch 1501 is 25.32224376670687 with r2-score 70.00429175455045\n",
      "Error at epoch 1601 is 24.775501911192073 with r2-score 70.65194009624913\n",
      "Error at epoch 1701 is 24.369576232800526 with r2-score 71.13278327628272\n",
      "Error at epoch 1801 is 24.061054280389364 with r2-score 71.4982459326374\n",
      "Error at epoch 1901 is 23.820590457276346 with r2-score 71.78308967508518\n",
      "Error at epoch 2001 is 23.628282884196135 with r2-score 72.01088946678802\n",
      "Error at epoch 2101 is 23.470577528156557 with r2-score 72.19770086834035\n",
      "Error at epoch 2201 is 23.33819364263547 with r2-score 72.35451747760584\n",
      "Error at epoch 2301 is 23.22473256114173 with r2-score 72.48891889673182\n",
      "Error at epoch 2401 is 23.125743968675405 with r2-score 72.6061767893026\n",
      "Error at epoch 2501 is 23.038098605204205 with r2-score 72.70999795049073\n",
      "Error at epoch 2601 is 22.95956634418502 with r2-score 72.8030240982164\n",
      "Error at epoch 2701 is 22.888532001853516 with r2-score 72.88716851399619\n",
      "Error at epoch 2801 is 22.82380357719757 with r2-score 72.96384319413336\n",
      "Error at epoch 2901 is 22.764482571665965 with r2-score 73.03411246375835\n",
      "Error at epoch 3001 is 22.70987604350225 with r2-score 73.09879715371656\n",
      "Error at epoch 3101 is 22.659436751510956 with r2-score 73.15854550384765\n",
      "Error at epoch 3201 is 22.612722231153764 with r2-score 73.21388163979081\n",
      "Error at epoch 3301 is 22.569366653559737 with r2-score 73.26523890766585\n",
      "Error at epoch 3401 is 22.52906133450362 with r2-score 73.31298296235093\n",
      "Error at epoch 3501 is 22.491541112958675 with r2-score 73.35742790289947\n",
      "Error at epoch 3601 is 22.456574726533898 with r2-score 73.39884767340867\n",
      "Error at epoch 3701 is 22.42395792063452 with r2-score 73.43748422563012\n",
      "Error at epoch 3801 is 22.393508437775797 with r2-score 73.47355345442634\n",
      "Error at epoch 3901 is 22.36506230894904 with r2-score 73.5072495908691\n",
      "Error at epoch 4001 is 22.338471054393466 with r2-score 73.53874851809206\n",
      "Error at epoch 4101 is 22.313599526138216 with r2-score 73.56821032692844\n",
      "Error at epoch 4201 is 22.290324209076182 with r2-score 73.59578132839044\n",
      "Error at epoch 4301 is 22.26853185441939 with r2-score 73.62159567242286\n",
      "Error at epoch 4401 is 22.248118358093734 with r2-score 73.6457766765114\n",
      "Error at epoch 4501 is 22.22898782295519 with r2-score 73.66843793654357\n",
      "Error at epoch 4601 is 22.211051761678103 with r2-score 73.68968427103495\n",
      "Error at epoch 4701 is 22.194228409488694 with r2-score 73.70961253523748\n",
      "Error at epoch 4801 is 22.178442124414318 with r2-score 73.7283123315799\n",
      "Error at epoch 4901 is 22.163622858620556 with r2-score 73.74586663590053\n"
     ]
    }
   ],
   "source": [
    "theta = gradient_descent(X, y, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.102831678096674"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_pred, y_true):\n",
    "    \n",
    "    e_pred = ((y_pred - y_true)**2).sum()\n",
    "    e_mean = ((y_true.mean() - y_true)**2).sum()\n",
    "    \n",
    "    return (1 - e_pred/e_mean)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score(hypothesis(X, np.random.randn(X.shape[1])), y) #random prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.10421201055735"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(hypothesis(X, theta), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
